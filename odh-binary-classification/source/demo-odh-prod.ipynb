{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka\n",
    "!pip install boto\n",
    "!rm -Rf data/production\n",
    "!rm -Rf data/model\n",
    "!mkdir -p data/production\n",
    "!mkdir -p data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import boto\n",
    "import boto.s3.connection\n",
    "\n",
    "predict_data_dir = \"data/predict\"\n",
    "models_data_dir = \"data/model\"\n",
    "\n",
    "# << YOUR ACCESS KEY HERE >>\n",
    "access_key = 'XXXXXXXXXXXXXXXX'\n",
    "# << YOUR SECRET KEY HERE >>\n",
    "secret_key = 'YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY'\n",
    "# << YOUR INTERNAL OCS S3 ENDPOINT HERE>>\n",
    "s3_gateway = 's3.openshift-storage.svc'\n",
    "s3_port = 443\n",
    "\n",
    "# do not verify ssl certificate\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "conn = boto.connect_s3(\n",
    "       aws_access_key_id = access_key,\n",
    "       aws_secret_access_key = secret_key,\n",
    "       host = s3_gateway,\n",
    "       port = s3_port,\n",
    "       calling_format = boto.s3.connection.OrdinaryCallingFormat(),\n",
    "       )\n",
    "\n",
    "def getDataFromS3(conn, bucket_name, directory, files):\n",
    "    # bind to bucket\n",
    "    bucket = conn.get_bucket(bucket_name, validate=False)\n",
    "\n",
    "    for item in files:\n",
    "        key = bucket.get_key(item)\n",
    "        key.get_contents_to_filename(os.path.join(directory, item))\n",
    "    return\n",
    "\n",
    "# filenames\n",
    "model_weights = 'cnn.h5'\n",
    "#model_json = 'well_trained_model.json'\n",
    "\n",
    "# download the model from ceph\n",
    "getDataFromS3(conn, 'models', models_data_dir, [model_weights])\n",
    "\n",
    "import json\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "IMAGE_SIZE = 200\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = IMAGE_SIZE, IMAGE_SIZE\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "class ConvNN:\n",
    "    \n",
    "    def __init__(self, image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT, epochs=32, batch_size=8, \n",
    "                 dropout=DROPOUT, learning_rate=LEARNING_RATE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3), \n",
    "                 training_dir='', validation_dir='', test_dir='', \n",
    "                 model_filename='cnn', verbose=True):\n",
    "        \n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.lr = learning_rate\n",
    "        self.input_shape = input_shape\n",
    "        self.training_dir = training_dir\n",
    "        self.validation_dir = validation_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.model_filename = model_filename\n",
    "        self.verbose = verbose\n",
    "        self.accuracy = ''\n",
    "\n",
    "    # create the convolutional neural network\n",
    "    def create(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Conv2D(32, 3, 3, border_mode='same', input_shape=self.input_shape, activation='relu'))\n",
    "        self.model.add(Conv2D(32, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(256, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(Dense(256, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                            optimizer=RMSprop(lr=self.lr),\n",
    "                            metrics=['accuracy'])\n",
    "        \n",
    "        with open(self.model_filename + '.txt', \"w\") as fh:\n",
    "            self.model.summary(print_fn=lambda line: fh.write(line + \"\\n\"))\n",
    "                    \n",
    "    def load(self, weights_filename):\n",
    "        self.model.load_weights(weights_filename)\n",
    "    \n",
    "    # evaluate model\n",
    "    def evaluate(self, test_size=1):\n",
    "        test_file = self.model_filename + 'test.txt'\n",
    "        test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_data_generator.flow_from_directory(\n",
    "                            self.test_dir,\n",
    "                            target_size=(self.image_width, self.image_height),\n",
    "                            batch_size=1,\n",
    "                            class_mode=\"binary\", \n",
    "                            shuffle=False)\n",
    "        \n",
    "        open(test_file, \"w\")\n",
    "        probabilities = self.model.predict_generator(test_generator, test_size)\n",
    "        for index, probability in enumerate(probabilities):\n",
    "            image_path = os.path.join(self.test_dir, test_generator.filenames[index]) \n",
    "            img = mpimg.imread(image_path)\n",
    "            with open(test_file, \"a\") as fh:\n",
    "                fh.write(str(probability[0]) + \" for: \" + image_path + \"\\n\")\n",
    "            plt.imshow(img)\n",
    "            if probability > 0.5:\n",
    "                plt.title(\"%.2f\" % (probability[0]*100) + \"% dog.\")\n",
    "            else:\n",
    "                plt.title(\"%.2f\" % ((1-probability[0])*100) + \"% cat.\")\n",
    "            plt.show()\n",
    "                          \n",
    "    def predict(self, images):       \n",
    "        return self.model.predict_classes(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "# you can install python modules if they are not available on the notebook\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# model creation\n",
    "model = ConvNN()\n",
    "\n",
    "# load weights\n",
    "weights_filename = os.path.join('data/model', model_weights)\n",
    "\n",
    "model.create()\n",
    "model.load(weights_filename)\n",
    "\n",
    "# << YOUR KAFKA ENDPOINT HERE >>\n",
    "kafkaendpoint =  'bootstrap_kafka_endpoint:443'\n",
    "\n",
    "def get_images_from_kafka(kafkaendpoint, topic, dest_dir):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=kafkaendpoint,\n",
    "                             auto_offset_reset='earliest',\n",
    "                             enable_auto_commit=False,\n",
    "                             ssl_context= ssl._create_unverified_context(),\n",
    "                             security_protocol='SSL',\n",
    "                             heartbeat_interval_ms=1000,\n",
    "                             group_id='consumer-group2')\n",
    "    \n",
    "    # assigning consumer to a kafka partition\n",
    "    tp = TopicPartition(topic=topic, partition=0)\n",
    "    consumer.assign([tp])\n",
    "    consumer.seek_to_beginning(tp)  \n",
    "    # lastOffset in topic\n",
    "    lastOffset = consumer.end_offsets([tp])[tp]\n",
    "\n",
    "    # read images\n",
    "    count = 1\n",
    "    for message in consumer:\n",
    "        # stop reading when there is no more messages on topic\n",
    "        if consumer.position(tp) == lastOffset:\n",
    "            image_filename = os.path.join(dest_dir, str(count)+'.jpg')\n",
    "            message = message.value\n",
    "            fd = open(image_filename,'wb')\n",
    "            fd.write(message)\n",
    "            fd.close()\n",
    "            break\n",
    "            \n",
    "        image_filename = os.path.join(dest_dir, str(count)+'.jpg')\n",
    "        message = message.value\n",
    "        fd = open(image_filename,'wb')\n",
    "        fd.write(message)\n",
    "        fd.close()\n",
    "        count += 1\n",
    "    \n",
    "    consumer.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "# read images from kafka and store it to local disk\n",
    "production_topic = 'predict'\n",
    "production_data_dir = 'data/production'\n",
    "get_images_from_kafka(kafkaendpoint, production_topic, production_data_dir)\n",
    "\n",
    "# predict images\n",
    "predict_images = [os.path.join(production_data_dir, f) for f in os.listdir(production_data_dir) \n",
    "                      if os.path.isfile(os.path.join(production_data_dir, f))]\n",
    "\n",
    "def classify(data, model):\n",
    "    dogs = []\n",
    "    cats = []\n",
    "    images = []\n",
    "    # reshape the image to be used in the neural network\n",
    "    for item in data:\n",
    "        image = Image.open(item)\n",
    "        image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        images.append(np.array(image))\n",
    "\n",
    "    images = np.array(images)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        images = images.reshape(images.shape[0], 3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    else:\n",
    "        images = images.reshape(images.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "               \n",
    "    return model.predict(images)\n",
    "\n",
    "# predict the data from kafka\n",
    "predictions = classify(predict_images, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 -> cat\n",
    "# 1 -> dog\n",
    "for item in range(0, len(predictions)):\n",
    "    if predictions[item][0] == 0:\n",
    "        plt.figure()\n",
    "        img = mpimg.imread(predict_images[item])\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(0, len(predictions)):\n",
    "    if predictions[item][0] == 1:\n",
    "        plt.figure()\n",
    "        img = mpimg.imread(predict_images[item])\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
