{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Binary Image Classification Problem\n",
    "\n",
    "This notebook is a modification of [image_classifier notebook](https://github.com/gsurma/image_classifier).\n",
    "\n",
    "This notebook trains a model to classify images, in this particular case to classify images into two categories:\n",
    "\n",
    "* Cats\n",
    "* Dogs\n",
    "\n",
    "This notebooks uses an included set of images to train and validate the model. Due to time the notebook uses a reduced set of images.\n",
    "\n",
    "The results of the model that we will build will not be accurate enough to be used but the goal of the lab is not to build accurrate models but illustrate how OCP and Red Hat Ceph Storage can be used together to run this kind of workloads.\n",
    "\n",
    "If you want to get an accurate model you can use [this set of images to train the model](https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765) (25000 images). Train the model using that set of images took 35 hours using a laptop with a 4-core **Intel(R) Core(TM) i7-4710HQ CPU @ 2.50GHz** (hyperthreading enabled).\n",
    "\n",
    "## Installing python modules\n",
    "\n",
    "We can install additional python modules, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka\n",
    "!pip install livelossplot\n",
    "!pip install boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notebook configuration\n",
    "\n",
    "Disabling scroll in output cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and preparing data\n",
    "\n",
    "We need first to get the data.\n",
    "\n",
    "Data will be downloaded from kafka and we will use three datasets:\n",
    "\n",
    "+ **Train** dataset to train the data.\n",
    "+ **Validation** dataset to validate the model while training.\n",
    "+ **Test** dataset to evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "TRAINING_LOGS_FILE = \"training_logs.csv\"\n",
    "MODEL_SUMMARY_FILE = \"model_summary.txt\"\n",
    "TEST_FILE = \"test_file.txt\"\n",
    "MODEL_FILE = \"model.h5\"\n",
    "\n",
    "# Data\n",
    "path = \"\"\n",
    "training_data_dir = path + \"data/training\" \n",
    "validation_data_dir = path + \"data/validation\" \n",
    "test_data_dir = path + \"data/test\" \n",
    "predict_data_dir = path + \"data/predict\"\n",
    "\n",
    "# Create directories on local storage\n",
    "# you can create directories on local storage to\n",
    "# adapt to your needs\n",
    "! mkdir -p {training_data_dir}\n",
    "! mkdir -p {training_data_dir}/cat\n",
    "! mkdir -p {training_data_dir}/dog\n",
    "! mkdir -p {validation_data_dir}\n",
    "! mkdir -p {validation_data_dir}/cat\n",
    "! mkdir -p {validation_data_dir}/dog\n",
    "! mkdir -p {test_data_dir}\n",
    "! mkdir -p {test_data_dir}/cat\n",
    "! mkdir -p {test_data_dir}/dog\n",
    "! mkdir -p {predict_data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the images from CEPH to local storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import boto\n",
    "import boto.s3.connection\n",
    "\n",
    "# index -> topic\n",
    "# value -> directory\n",
    "data = {}\n",
    "data['train-cat'] = os.path.join(training_data_dir,'cat')\n",
    "data['train-dog'] = os.path.join(training_data_dir,'dog')\n",
    "data['validation-cat'] = os.path.join(validation_data_dir,'cat')\n",
    "data['validation-dog'] = os.path.join(validation_data_dir,'dog')\n",
    "data['test-cat'] = os.path.join(test_data_dir,'cat')\n",
    "data['test-dog'] = os.path.join(test_data_dir,'dog')\n",
    "\n",
    "# << YOUR ACCESS KEY HERE >>\n",
    "access_key = 'XXXXXXXXXXXXXX'\n",
    "# << YOUR SECRET KEY HERE >>\n",
    "secret_key = 'YYYYYYYYYYYYYYYYYYYYYYYYYY'\n",
    "# << YOUR INTERNAL OCS S3 ENDPOINT HERE>>\n",
    "s3_gateway = 's3.openshift-storage.svc'\n",
    "s3_port = 443\n",
    "\n",
    "# do not verify ssl certificate\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "conn = boto.connect_s3(\n",
    "       aws_access_key_id = access_key,\n",
    "       aws_secret_access_key = secret_key,\n",
    "       host = s3_gateway,\n",
    "       port = s3_port,\n",
    "       calling_format = boto.s3.connection.OrdinaryCallingFormat(),\n",
    "       )\n",
    "\n",
    "def getDataFromS3(conn, bucket_name, directory, files):\n",
    "    # bind to bucket\n",
    "    bucket = conn.get_bucket(bucket_name, validate=False)\n",
    "\n",
    "    for item in files:\n",
    "        print(\"Descargando objeto %s del bucket %s.\" % (item, bucket_name))\n",
    "        print(os.path.join(directory,item))\n",
    "        key = bucket.get_key(item)\n",
    "        key.get_contents_to_filename(os.path.join(directory,item))\n",
    "    return\n",
    "\n",
    "datafiles = ['1.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg', '10.jpg']\n",
    "\n",
    "# download the data from ceph\n",
    "for item in data.keys():\n",
    "    getDataFromS3(conn, item, data[item], datafiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convolutional neural network as a python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "# Hyperparams\n",
    "IMAGE_SIZE = 200\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = IMAGE_SIZE, IMAGE_SIZE\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "\n",
    "# a class implementing a convolutional neural network\n",
    "class ConvNN:\n",
    "    \n",
    "    def __init__(self, image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT, epochs=32, batch_size=8, \n",
    "                 dropout=DROPOUT, learning_rate=LEARNING_RATE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3), \n",
    "                 training_dir=training_data_dir, validation_dir=validation_data_dir, test_dir=test_data_dir, \n",
    "                 model_filename='cnn', verbose=True):\n",
    "        \n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.lr = learning_rate\n",
    "        self.input_shape = input_shape\n",
    "        self.training_dir = training_dir\n",
    "        self.validation_dir = validation_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.model_filename = model_filename\n",
    "        self.verbose = verbose\n",
    "        self.accuracy = ''\n",
    "    \n",
    "    # create the convolutional neural network\n",
    "    def create(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Conv2D(32, 3, 3, border_mode='same', input_shape=self.input_shape, activation='relu'))\n",
    "        self.model.add(Conv2D(32, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(Conv2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(256, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(Dense(256, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                            optimizer=RMSprop(lr=self.lr),\n",
    "                            metrics=['accuracy'])\n",
    "        \n",
    "        with open(self.model_filename + '.txt', \"w\") as fh:\n",
    "            self.model.summary(print_fn=lambda line: fh.write(line + \"\\n\"))\n",
    "            \n",
    "    # train the model\n",
    "    def train(self):\n",
    "        # Data augmentation\n",
    "        training_data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True)\n",
    "        validation_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Data preparation\n",
    "        training_generator = training_data_generator.flow_from_directory(\n",
    "            self.training_dir,\n",
    "            target_size=(self.image_width, self.image_height),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode=\"binary\")\n",
    "        validation_generator = validation_data_generator.flow_from_directory(\n",
    "            self.validation_dir,\n",
    "            target_size=(self.image_width, self.image_height),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode=\"binary\")\n",
    "\n",
    "        # Training\n",
    "        self.model.fit_generator(\n",
    "                    training_generator,\n",
    "                    steps_per_epoch=len(training_generator.filenames) // self.batch_size,\n",
    "                    epochs=self.epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(validation_generator.filenames) // self.batch_size,\n",
    "                    callbacks=[PlotLossesKeras(), CSVLogger(self.model_filename + '_logs.csv',\n",
    "                                                            append=False,\n",
    "                                                            separator=\";\")], \n",
    "                    verbose=self.verbose)\n",
    "        \n",
    "    # save the trained model\n",
    "    def save(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "    \n",
    "    # load a previously trained model\n",
    "    def load(self, filename):\n",
    "        self.model.load_weights(filename)\n",
    "    \n",
    "    # evaluate model\n",
    "    def evaluate(self, test_size=1):\n",
    "        test_file = self.model_filename + 'test.txt'\n",
    "        test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_data_generator.flow_from_directory(\n",
    "                            self.test_dir,\n",
    "                            target_size=(self.image_width, self.image_height),\n",
    "                            batch_size=1,\n",
    "                            class_mode=\"binary\", \n",
    "                            shuffle=False)\n",
    "        \n",
    "        open(test_file, \"w\")\n",
    "        probabilities = self.model.predict_generator(test_generator, test_size)\n",
    "        for index, probability in enumerate(probabilities):\n",
    "            image_path = os.path.join(self.test_dir, test_generator.filenames[index]) \n",
    "            img = mpimg.imread(image_path)\n",
    "            with open(test_file, \"a\") as fh:\n",
    "                fh.write(str(probability[0]) + \" for: \" + image_path + \"\\n\")\n",
    "            plt.imshow(img)\n",
    "            if probability > 0.5:\n",
    "                plt.title(\"%.2f\" % (probability[0]*100) + \"% dog.\")\n",
    "            else:\n",
    "                plt.title(\"%.2f\" % ((1-probability[0])*100) + \"% cat.\")\n",
    "            plt.show()\n",
    "                          \n",
    "    def predict(self, images):       \n",
    "        return self.model.predict_classes(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The first step is to train the model.\n",
    "\n",
    "You only will have to click in the bellow Cell and press **Shift + Enter** to run the cell.\n",
    "\n",
    "This will start the model training and you could see two charts:\n",
    "\n",
    "* Log-loss (cost function)\n",
    "* Accuracy\n",
    "\n",
    "You will have to wait until it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_trained_model = ConvNN(epochs=4, model_filename='poorly_trained_model')\n",
    "\n",
    "poorly_trained_model.create()\n",
    "\n",
    "poorly_trained_model.train()\n",
    "\n",
    "poorly_trained_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_trained_model.evaluate(test_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a well trained model\n",
    "\n",
    "As we can see the accuracy of the model can be \"improved\". We train the model with a small dataset so the model will be a bad model as we can see.\n",
    "\n",
    "Now we will load a better model which was previously trained with a bigger dataset: 25000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_trained_model_filename = 'well-trained-model.h5'\n",
    "well_trained_model = ConvNN(epochs=2, model_filename='well_trained_model')\n",
    "\n",
    "well_trained_model.create()\n",
    "\n",
    "well_trained_model.load(well_trained_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_trained_model.evaluate(test_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Predictions using new images from kafka topic **predict**. So we read kafka topic and store the images on local disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "# you can install python modules if they are not available on the notebook\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# << YOUR KAFKA ENDPOINT HERE >>\n",
    "kafkaendpoint = 'bootstrap_kafka_endpoint:443'\n",
    "\n",
    "def get_images_from_kafka(kafkaendpoint, topic, dest_dir):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=kafkaendpoint,\n",
    "                             auto_offset_reset='earliest',\n",
    "                             enable_auto_commit=False,\n",
    "                             ssl_context= ssl._create_unverified_context(),\n",
    "                             security_protocol='SSL',\n",
    "                             heartbeat_interval_ms=1000,\n",
    "                             group_id='consumer-group2')\n",
    "    \n",
    "    # assigning consumer to a kafka partition\n",
    "    tp = TopicPartition(topic=topic, partition=0)\n",
    "    consumer.assign([tp])\n",
    "    consumer.seek_to_beginning(tp)  \n",
    "    # lastOffset in topic\n",
    "    lastOffset = consumer.end_offsets([tp])[tp]\n",
    "\n",
    "    # read images\n",
    "    count = 1\n",
    "    for message in consumer:\n",
    "        # stop reading when there is no more messages on topic\n",
    "        if consumer.position(tp) == lastOffset:\n",
    "            image_filename = os.path.join(dest_dir, str(count)+'.jpg')\n",
    "            message = message.value\n",
    "            fd = open(image_filename,'wb')\n",
    "            fd.write(message)\n",
    "            fd.close()\n",
    "            break\n",
    "            \n",
    "        image_filename = os.path.join(dest_dir, str(count)+'.jpg')\n",
    "        message = message.value\n",
    "        fd = open(image_filename,'wb')\n",
    "        fd.write(message)\n",
    "        fd.close()\n",
    "        count += 1\n",
    "    \n",
    "    consumer.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "# read images from kafka and store it to local disk\n",
    "predict_topic = 'predict'\n",
    "get_images_from_kafka(kafkaendpoint, predict_topic, predict_data_dir)\n",
    "\n",
    "# predict images\n",
    "predict_images = [os.path.join(predict_data_dir, f) for f in os.listdir(predict_data_dir) \n",
    "                      if os.path.isfile(os.path.join(predict_data_dir, f))]\n",
    "\n",
    "def classify(data, model):\n",
    "    dogs = []\n",
    "    cats = []\n",
    "    images = []\n",
    "    # reshape the image to be used in the neural network\n",
    "    for item in data:\n",
    "        image = Image.open(item)\n",
    "        image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        images.append(np.array(image))\n",
    "\n",
    "    images = np.array(images)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        images = images.reshape(images.shape[0], 3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "    else:\n",
    "        images = images.reshape(images.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "               \n",
    "    return model.predict(images)\n",
    "\n",
    "# predict the data from kafka\n",
    "predictions = classify(predict_images, well_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 0 -> cat\n",
    "# 1 -> dog\n",
    "for item in range(0, len(predictions)):\n",
    "    if predictions[item][0] == 0:\n",
    "        plt.figure()\n",
    "        img = mpimg.imread(predict_images[item])\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(0, len(predictions)):\n",
    "    if predictions[item][0] == 1:\n",
    "        plt.figure()\n",
    "        img = mpimg.imread(predict_images[item])\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model to Red Hat Ceph Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadDataToS3(conn, bucket_name, weights_file):\n",
    "    # bind to bucket\n",
    "    bucket = conn.get_bucket(bucket_name, validate=False)\n",
    "\n",
    "    # upload the file\n",
    "    key = bucket.new_key(weights_file)\n",
    "    key.set_contents_from_filename(weights_file)\n",
    "    \n",
    "    return\n",
    "\n",
    "# saving the model weights\n",
    "well_trained_model.save('cnn.h5')\n",
    "\n",
    "# uploading data to S3\n",
    "uploadDataToS3(conn, 'models', 'cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
